{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "This tutorial will walk you through the comprehensive process of training a model in LibRecommender, i.e. **data processing -> feature engineering -> training -> evaluate -> save/load -> retrain**. We will use [Wide & Deep](https://arxiv.org/pdf/1606.07792.pdf) as the example algorithm. \n",
    "\n",
    "First make sure LibRecommender is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install LibRecommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For how to deploy a trained model in LibRecommender, see [Serving Guide](https://librecommender.readthedocs.io/en/latest/serving_guide/python.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**NOTE**: If you encounter errors like `Variables already exist, disallowed...`, just call `tf.compat.v1.reset_default_graph()` first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this tutorial we willl use the [MovieLens 1M](https://grouplens.org/datasets/movielens/1m/) dataset. The following code will load the data into `pandas.DataFrame` format. If the data does not exist locally, it will be downloaded at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_ml_1m():\n",
    "    # download and extract zip file\n",
    "    tf.keras.utils.get_file(\n",
    "        \"ml-1m.zip\", \n",
    "        \"http://files.grouplens.org/datasets/movielens/ml-1m.zip\", \n",
    "        cache_dir=\".\", \n",
    "        cache_subdir=\".\", \n",
    "        extract=True,\n",
    "    )\n",
    "    # read and merge data into same table\n",
    "    cur_path = Path(\".\").absolute()\n",
    "    ratings = pd.read_csv(\n",
    "        cur_path / \"ml-1m\" / \"ratings.dat\", \n",
    "        sep=\"::\", \n",
    "        usecols=[0, 1, 2, 3], \n",
    "        names=[\"user\", \"item\", \"rating\", \"time\"],\n",
    "    )\n",
    "    users = pd.read_csv(\n",
    "        cur_path / \"ml-1m\" / \"users.dat\", \n",
    "        sep=\"::\",\n",
    "        usecols=[0, 1, 2, 3], \n",
    "        names=[\"user\", \"sex\", \"age\", \"occupation\"],\n",
    "    )\n",
    "    items = pd.read_csv(\n",
    "        cur_path / \"ml-1m\" / \"movies.dat\", \n",
    "        sep=\"::\",\n",
    "        usecols=[0, 2], \n",
    "        names=[\"item\", \"genre\"],\n",
    "        encoding=\"iso-8859-1\",\n",
    "    )\n",
    "    items[[\"genre1\", \"genre2\", \"genre3\"]] = (\n",
    "        items[\"genre\"].str.split(r\"|\", expand=True).fillna(\"missing\").iloc[:, :3]\n",
    "    )\n",
    "    items.drop(\"genre\", axis=1, inplace=True)\n",
    "    data = ratings.merge(users, on=\"user\").merge(items, on=\"item\")\n",
    "    data.rename(columns={\"rating\": \"label\"}, inplace=True)\n",
    "    # random shuffle data\n",
    "    data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (1000209, 10)\n"
     ]
    }
   ],
   "source": [
    "data = load_ml_1m()\n",
    "print(\"data shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>label</th>\n",
       "      <th>time</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>genre1</th>\n",
       "      <th>genre2</th>\n",
       "      <th>genre3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>389991</th>\n",
       "      <td>3993</td>\n",
       "      <td>2119</td>\n",
       "      <td>4</td>\n",
       "      <td>965699325</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>Horror</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155010</th>\n",
       "      <td>699</td>\n",
       "      <td>564</td>\n",
       "      <td>2</td>\n",
       "      <td>975558670</td>\n",
       "      <td>M</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737170</th>\n",
       "      <td>4121</td>\n",
       "      <td>2377</td>\n",
       "      <td>4</td>\n",
       "      <td>965357614</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>Horror</td>\n",
       "      <td>Sci-Fi</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78641</th>\n",
       "      <td>4330</td>\n",
       "      <td>2028</td>\n",
       "      <td>4</td>\n",
       "      <td>965242550</td>\n",
       "      <td>F</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>Action</td>\n",
       "      <td>Drama</td>\n",
       "      <td>War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629621</th>\n",
       "      <td>1065</td>\n",
       "      <td>597</td>\n",
       "      <td>3</td>\n",
       "      <td>974948100</td>\n",
       "      <td>F</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>Romance</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787474</th>\n",
       "      <td>5831</td>\n",
       "      <td>708</td>\n",
       "      <td>4</td>\n",
       "      <td>957909659</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>Romance</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344925</th>\n",
       "      <td>4092</td>\n",
       "      <td>3526</td>\n",
       "      <td>5</td>\n",
       "      <td>965420545</td>\n",
       "      <td>F</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>Drama</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888858</th>\n",
       "      <td>3681</td>\n",
       "      <td>441</td>\n",
       "      <td>5</td>\n",
       "      <td>966356720</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417010</th>\n",
       "      <td>771</td>\n",
       "      <td>908</td>\n",
       "      <td>4</td>\n",
       "      <td>975440099</td>\n",
       "      <td>F</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511404</th>\n",
       "      <td>2795</td>\n",
       "      <td>1179</td>\n",
       "      <td>3</td>\n",
       "      <td>972892354</td>\n",
       "      <td>M</td>\n",
       "      <td>50</td>\n",
       "      <td>13</td>\n",
       "      <td>Crime</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Film-Noir</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user  item  label       time sex  age  occupation  genre1    genre2  \\\n",
       "389991  3993  2119      4  965699325   M   25           0  Horror   missing   \n",
       "155010   699   564      2  975558670   M   18           0  Comedy   missing   \n",
       "737170  4121  2377      4  965357614   M   45           7  Horror    Sci-Fi   \n",
       "78641   4330  2028      4  965242550   F   45           6  Action     Drama   \n",
       "629621  1065   597      3  974948100   F   25           0  Comedy   Romance   \n",
       "787474  5831   708      4  957909659   M   25           1  Comedy   Romance   \n",
       "344925  4092  3526      5  965420545   F   25           0  Comedy     Drama   \n",
       "888858  3681   441      5  966356720   M   25           7  Comedy   missing   \n",
       "417010   771   908      4  975440099   F   50           1   Drama  Thriller   \n",
       "511404  2795  1179      3  972892354   M   50          13   Crime     Drama   \n",
       "\n",
       "           genre3  \n",
       "389991    missing  \n",
       "155010    missing  \n",
       "737170    missing  \n",
       "78641         War  \n",
       "629621    missing  \n",
       "787474    missing  \n",
       "344925    missing  \n",
       "888858    missing  \n",
       "417010    missing  \n",
       "511404  Film-Noir  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[random.choices(range(len(data)), k=10)]  # randomly select 10 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we have about 1 million data. In order to perform evaluation after training, we need to split the data into train, eval and test data first. In this tutorial we will simply use `random_split`. For other ways of splitting data, see [Data Processing](https://librecommender.readthedocs.io/en/latest/user_guide/data_processing.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "**For now, We will only use first half data for training. Later we will use the rest data to retrain the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Process Data & Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from libreco.data import random_split\n",
    "\n",
    "# split data into three folds for training, evaluating and testing\n",
    "first_half_data = data[: (len(data) // 2)]\n",
    "train_data, eval_data, test_data = random_split(first_half_data, multi_ratios=[0.8, 0.1, 0.1], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The data contains some categorical features such as \"sex\" and \"genre\", as well as a numerical feature \"age\". In LibRecommender we use `sparse_col` to represent categorical features and `dense_col` to represent numerical features. So one should specify the column information and then use `DatasetFeat.build_*` functions to process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first half data shape: (500104, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"first half data shape:\", first_half_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from libreco.data import DatasetFeat\n",
    "\n",
    "sparse_col = [\"sex\", \"occupation\", \"genre1\", \"genre2\", \"genre3\"]\n",
    "dense_col = [\"age\"]\n",
    "user_col = [\"sex\", \"age\", \"occupation\"]\n",
    "item_col = [\"genre1\", \"genre2\", \"genre3\"]\n",
    "\n",
    "train_data, data_info = DatasetFeat.build_trainset(train_data, user_col, item_col, sparse_col, dense_col)\n",
    "eval_data = DatasetFeat.build_evalset(eval_data)\n",
    "test_data = DatasetFeat.build_testset(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\"user_col\" means features belong to user, and \"item_col\" means features belong to item. Note that the column numbers should match, i.e. `len(sparse_col) + len(dense_col) == len(user_col) + len(item_col)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_users: 6040, n_items: 3576, data density: 1.8523 %\n"
     ]
    }
   ],
   "source": [
    "print(data_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this example we treat all the samples in data as positive samples, and perform negative sampling. This is called \"implicit data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random neg item sampling elapsed: 0.338s\n",
      "random neg item sampling elapsed: 0.043s\n",
      "random neg item sampling elapsed: 0.043s\n"
     ]
    }
   ],
   "source": [
    "# sample negative items for each record\n",
    "train_data.build_negative_samples(data_info)\n",
    "eval_data.build_negative_samples(data_info)\n",
    "test_data.build_negative_samples(data_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now with all the data and features prepared, we can start training the model! \n",
    "\n",
    "Since as its name suggests, the `Wide & Deep` algorithm has wide and deep parts, and they use different optimizers. So we should specify the learning rate separately by using a dict: `{\"wide\": 0.01, \"deep\": 3e-4}`. For other model hyperparameters, see API reference of [WideDeep](https://librecommender.readthedocs.io/en/latest/api/algorithms/wide_deep.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from libreco.algorithms import WideDeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start time: \u001b[35m2023-02-20 19:46:23\u001b[0m\n",
      "total params: \u001b[33m192,477\u001b[0m | embedding params: \u001b[33m165,109\u001b[0m | network params: \u001b[33m27,368\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|█████████████████████████████████████████████████| 391/391 [00:04<00:00, 95.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 elapsed: 4.280s\n",
      "\t \u001b[32mtrain_loss: 8.1818\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|██████████████████████████████████████████| 13/13 [00:00<00:00, 92.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval log_loss: 5.8765\n",
      "\t eval roc_auc: 0.7034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_listwise: 100%|██████████████████████████████████████| 2797/2797 [00:15<00:00, 184.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval precision@10: 0.0287\n",
      "\t eval recall@10: 0.0413\n",
      "\t eval ndcg@10: 0.1096\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|████████████████████████████████████████████████| 391/391 [00:03<00:00, 104.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 elapsed: 3.925s\n",
      "\t \u001b[32mtrain_loss: 4.1555\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|█████████████████████████████████████████| 13/13 [00:00<00:00, 184.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval log_loss: 2.5937\n",
      "\t eval roc_auc: 0.8073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_listwise: 100%|██████████████████████████████████████| 2797/2797 [00:15<00:00, 184.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval precision@10: 0.0311\n",
      "\t eval recall@10: 0.0481\n",
      "\t eval ndcg@10: 0.1240\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "model = WideDeep(\n",
    "    task=\"ranking\",\n",
    "    data_info=data_info,\n",
    "    embed_size=16,\n",
    "    n_epochs=2,\n",
    "    loss_type=\"cross_entropy\",\n",
    "    lr={\"wide\": 0.005, \"deep\": 1e-4},\n",
    "    batch_size=2048,\n",
    "    use_bn=True,\n",
    "    hidden_units=(128, 64, 32),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_data,\n",
    "    verbose=2,\n",
    "    shuffle=True,\n",
    "    eval_data=eval_data,\n",
    "    metrics=[\"loss\", \"roc_auc\", \"precision\", \"recall\", \"ndcg\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We've trained the model for 2 epochs and evaluated the performance on the eval data during training. Next we can evaluate on the *independent* test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|█████████████████████████████████████████| 13/13 [00:00<00:00, 167.36it/s]\n",
      "eval_listwise: 100%|██████████████████████████████████████| 1024/1024 [00:05<00:00, 185.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 2.59511655229092,\n",
       " 'roc_auc': 0.8060875394289854,\n",
       " 'precision': 0.028857421875000004,\n",
       " 'recall': 0.046256462484535465,\n",
       " 'ndcg': 0.12245855533553587}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from libreco.evaluation import evaluate\n",
    "\n",
    "evaluate(model=model, data=test_data, metrics=[\"loss\", \"roc_auc\", \"precision\", \"recall\", \"ndcg\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Make Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The recommend part is pretty straightforward. You can make recommendation for one user or a batch of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([ 260, 2858,  858])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.recommend_user(user=1, n_rec=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([ 260, 2858,  858]),\n",
       " 2: array([1617, 2987,  608]),\n",
       " 3: array([1580, 1198,  480])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.recommend_user(user=[1, 2, 3], n_rec=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Save, Load and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "When saving the model, we should also save the `data_info` for feature information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_info.save(\"model_path\", model_name=\"wide_deep\")\n",
    "model.save(\"model_path\", model_name=\"wide_deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then we can load the model and make recommendation again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()  # need to reset graph in TensorFlow1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total params: \u001b[33m192,477\u001b[0m | embedding params: \u001b[33m165,109\u001b[0m | network params: \u001b[33m27,368\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: array([ 260, 2858,  858])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from libreco.data import DataInfo\n",
    "\n",
    "loaded_data_info = DataInfo.load(\"model_path\", model_name=\"wide_deep\")\n",
    "loaded_model = WideDeep.load(\"model_path\", model_name=\"wide_deep\", data_info=loaded_data_info)\n",
    "loaded_model.recommend_user(user=1, n_rec=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Retrain the Model with New Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Remember that we split the original `MovieLens 1M` data into two parts in the first place? We will treat the **second half** of the data as our new data and retrain the saved model with it. In real-world recommender systems, data may be generated every day, so it is inefficient to train the model from scratch every time we get some new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "second_half_data = data[(len(data) // 2) :]\n",
    "train_data, eval_data = random_split(second_half_data, multi_ratios=[0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second half data shape: (500105, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"second half data shape:\", second_half_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The data processing is similar, except that we should use `merge_trainset()` and `merge_evalset()` in DatasetFeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The purpose of these functions is combining information from old data with that from new data, especially for the possible new users/items from new data. For more details, see [Model Retrain](https://librecommender.readthedocs.io/en/latest/user_guide/model_retrain.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random neg item sampling elapsed: 0.346s\n",
      "random neg item sampling elapsed: 0.091s\n"
     ]
    }
   ],
   "source": [
    "train_data = DatasetFeat.merge_trainset(train_data, loaded_data_info, merge_behavior=True)  # use loaded_data_info\n",
    "eval_data = DatasetFeat.merge_evalset(eval_data, loaded_data_info)\n",
    "\n",
    "train_data.build_negative_samples(loaded_data_info, seed=2022)  # use loaded_data_info\n",
    "eval_data.build_negative_samples(loaded_data_info, seed=2222)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then we construct a new model, and call `rebuild_model` method to assign the old variables into the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()  # need to reset graph in TensorFlow1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total params: \u001b[33m194,228\u001b[0m | embedding params: \u001b[33m166,860\u001b[0m | network params: \u001b[33m27,368\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "new_model = WideDeep(\n",
    "    task=\"ranking\",\n",
    "    data_info=loaded_data_info,  # pass loaded_data_info\n",
    "    embed_size=16,\n",
    "    n_epochs=2,\n",
    "    loss_type=\"cross_entropy\",\n",
    "    lr={\"wide\": 0.005, \"deep\": 1e-4},\n",
    "    batch_size=2048,\n",
    "    use_bn=True,\n",
    "    hidden_units=(128, 64, 32),\n",
    ")\n",
    "\n",
    "new_model.rebuild_model(path=\"model_path\", model_name=\"wide_deep\", full_assign=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, the training and recommendation parts are the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start time: \u001b[35m2023-02-20 19:48:06\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|█████████████████████████████████████████████████| 391/391 [00:04<00:00, 96.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 elapsed: 4.258s\n",
      "\t \u001b[32mtrain_loss: 2.2819\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|█████████████████████████████████████████| 25/25 [00:00<00:00, 117.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval log_loss: 1.4160\n",
      "\t eval roc_auc: 0.8295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_listwise: 100%|██████████████████████████████████████| 2981/2981 [00:16<00:00, 176.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval precision@10: 0.0938\n",
      "\t eval recall@10: 0.0672\n",
      "\t eval ndcg@10: 0.2890\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|████████████████████████████████████████████████| 391/391 [00:03<00:00, 102.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 elapsed: 4.012s\n",
      "\t \u001b[32mtrain_loss: 1.4704\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_pointwise: 100%|█████████████████████████████████████████| 25/25 [00:00<00:00, 142.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval log_loss: 0.9665\n",
      "\t eval roc_auc: 0.8387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_listwise: 100%|██████████████████████████████████████| 2981/2981 [00:18<00:00, 162.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t eval precision@10: 0.0918\n",
      "\t eval recall@10: 0.0649\n",
      "\t eval ndcg@10: 0.2828\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "new_model.fit(\n",
    "    train_data, \n",
    "    verbose=2, \n",
    "    shuffle=True, \n",
    "    eval_data=eval_data,\n",
    "    metrics=[\"loss\", \"roc_auc\", \"precision\", \"recall\", \"ndcg\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([2858, 1580,  260])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.recommend_user(user=1, n_rec=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([2858, 1580,  260]),\n",
       " 2: array([ 260, 1196, 1580]),\n",
       " 3: array([1580, 2858,  589])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.recommend_user(user=[1, 2, 3], n_rec=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**This completes our tutorial!**\n",
    "\n",
    "+ For more examples, see the [examples](https://github.com/massquantity/LibRecommender/tree/master/examples) folder on GitHub. \n",
    "\n",
    "+ For more usages, please head to [User Guide](https://librecommender.readthedocs.io/en/latest/user_guide/index.html).\n",
    "\n",
    "+ For serving a trained model, please head to [Python Serving Guide](https://librecommender.readthedocs.io/en/latest/serving_guide/python.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
